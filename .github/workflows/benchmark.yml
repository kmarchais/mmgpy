name: Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      calibration_runs:
        description: "Number of calibration runs (only for calibration mode)"
        required: false
        default: "10"
      mode:
        description: "Mode: benchmark (default) or calibrate"
        required: false
        default: "benchmark"
        type: choice
        options:
          - benchmark
          - calibrate

env:
  VTK_VERSION_MAJOR: 9
  VTK_VERSION_MINOR: 5
  VTK_VERSION_PATCH: 2
  # Alert threshold calibrated for CI runner variance
  # Based on statistical analysis: μ + 3σ approach with safety margin
  # Higher thresholds reduce false positives from runner variability
  # Remeshing operations have highest variance (15-30% CV typical)
  BENCHMARK_ALERT_THRESHOLD: "200%"

permissions:
  contents: write
  deployments: write
  pull-requests: write

jobs:
  benchmark:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.mode != 'calibrate' }}

    steps:
      - uses: actions/checkout@v4

      - name: Install VTK
        run: |
          sudo apt-get update
          sudo apt-get install -y libvtk9-dev qtbase5-dev qt5-qmake libgl1-mesa-dev libglu1-mesa-dev freeglut3-dev

      - name: Set CMake path
        run: echo "CMAKE_PREFIX_PATH=/usr/lib/cmake/vtk" >> $GITHUB_ENV

      - uses: astral-sh/setup-uv@v4
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: uv sync -v

      - name: Run benchmarks
        run: |
          uv run pytest benchmarks/ \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --benchmark-group-by=group \
            --benchmark-sort=mean \
            --benchmark-warmup=on \
            --benchmark-min-rounds=3

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        if: github.event_name != 'pull_request'
        with:
          tool: "pytest"
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: ${{ env.BENCHMARK_ALERT_THRESHOLD }}
          comment-on-alert: true
          fail-on-alert: false

      - name: Compare benchmarks (PR only)
        uses: benchmark-action/github-action-benchmark@v1
        if: github.event_name == 'pull_request'
        with:
          tool: "pytest"
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: false
          alert-threshold: ${{ env.BENCHMARK_ALERT_THRESHOLD }}
          comment-on-alert: true
          fail-on-alert: false
          comment-always: false

      - name: Generate benchmark summary
        if: github.event_name == 'pull_request'
        id: summary
        run: |
          python3 << 'EOF'
          import json
          import os

          with open("benchmark-results.json") as f:
              data = json.load(f)

          benchmarks = data.get("benchmarks", [])
          if not benchmarks:
              print("No benchmark results found")
              exit(0)

          # Group by benchmark group
          groups = {}
          for b in benchmarks:
              group = b.get("group", "default")
              if group not in groups:
                  groups[group] = []
              groups[group].append(b)

          # Generate summary
          summary_lines = ["### Benchmark Results Summary\n"]
          summary_lines.append("| Group | Benchmarks | Mean Time Range |")
          summary_lines.append("|-------|------------|-----------------|")

          for group, benches in sorted(groups.items()):
              times = [b["stats"]["mean"] * 1000 for b in benches]
              min_t, max_t = min(times), max(times)
              summary_lines.append(f"| {group} | {len(benches)} | {min_t:.2f}ms - {max_t:.2f}ms |")

          summary_lines.append(f"\n*Total: {len(benchmarks)} benchmarks*")
          summary_lines.append(f"\n<details><summary>Alert threshold: {os.environ.get('BENCHMARK_ALERT_THRESHOLD', '200%')}</summary>")
          summary_lines.append("\nThis threshold accounts for GitHub Actions runner variability.")
          summary_lines.append("See `benchmarks/README.md` for calibration details.")
          summary_lines.append("</details>")

          summary = "\n".join(summary_lines)
          print(summary)

          # Write to GitHub step output
          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write("summary<<EOF\n")
              f.write(summary)
              f.write("\nEOF\n")
          EOF

      - name: Comment benchmark summary on PR
        if: github.event_name == 'pull_request' && steps.summary.outputs.summary
        uses: actions/github-script@v7
        with:
          script: |
            const summary = `${{ steps.summary.outputs.summary }}`;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' &&
              c.body.includes('### Benchmark Results Summary')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: summary
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: summary
              });
            }

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-results.json
          retention-days: 30

  calibrate:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.mode == 'calibrate' }}

    steps:
      - uses: actions/checkout@v4

      - name: Install VTK
        run: |
          sudo apt-get update
          sudo apt-get install -y libvtk9-dev qtbase5-dev qt5-qmake libgl1-mesa-dev libglu1-mesa-dev freeglut3-dev

      - name: Set CMake path
        run: echo "CMAKE_PREFIX_PATH=/usr/lib/cmake/vtk" >> $GITHUB_ENV

      - uses: astral-sh/setup-uv@v4
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: uv sync -v

      - name: Run benchmark calibration
        run: |
          uv run python scripts/calibrate_benchmarks.py \
            --runs ${{ github.event.inputs.calibration_runs || '10' }} \
            --output calibration-results.json

      - name: Analyze calibration results
        run: |
          uv run python scripts/analyze_benchmark_variance.py calibration-results.json

      - name: Upload calibration results
        uses: actions/upload-artifact@v4
        with:
          name: calibration-results
          path: calibration-results.json
          retention-days: 90
